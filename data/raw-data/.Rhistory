# Load the necessary libraries
library(httr)
library(jsonlite)
# Replace 'YOUR_API_KEY' with your actual API key from OpenWeatherMap
API_KEY <- 'YOUR_API_KEY'
# Specify the city and country code (e.g., 'New York,US')
city_name <- 'New York'
# Replace 'YOUR_API_KEY' with your actual API key from OpenWeatherMap
API_KEY <- '1ecdc1e0e3112db35cbc433886712dd1'
# Specify the city and country code (e.g., 'New York,US')
city_name <- 'New York'
country_code <- 'US'
# Define the base URL for the OpenWeatherMap API
base_url <- 'http://api.openweathermap.org/data/2.5/weather?'
# Construct the complete API URL with query parameters
api_url <- paste0(base_url, 'q=', URLencode(paste(city_name, country_code, sep = ',')), '&appid=', API_KEY)
# Send an HTTP GET request to the API
response <- GET(api_url)
# Check if the request was successful (status code 200)
if (http_status(response)$status_code == 200) {
# Parse the JSON response
data <- fromJSON(content(response, "text"), flatten = TRUE)
# Extract relevant weather information from the JSON data
temperature_kelvin <- data$main$temp
temperature_celsius <- temperature_kelvin - 273.15  # Convert to Celsius
weather_description <- data$weather$description
# Print the weather information
cat('City:', city_name, ', Country:', country_code, '\n')
cat('Temperature:', sprintf('%.2f°C', temperature_celsius), '\n')
cat('Weather:', weather_description, '\n')
} else {
# Print an error message if the request was not successful
cat('Error: Unable to retrieve weather data.\n')
}
View(response)
install.packages("rnoaa")
library(rnoaa)
# Define a list of states and their corresponding coordinates
states <- list(
California = c(lon = -119.4179, lat = 36.7783),  # California coordinates
Texas = c(lon = -99.9018, lat = 31.9686),        # Texas coordinates
Florida = c(lon = -81.5158, lat = 27.7663),      # Florida coordinates
Colorado = c(lon = -105.7821, lat = 39.5501),    # Colorado coordinates
Arizona = c(lon = -111.0937, lat = 34.0489),     # Arizona coordinates
Oregon = c(lon = -120.5542, lat = 43.8041),      # Oregon coordinates
Washington = c(lon = -120.7401, lat = 47.7511)   # Washington coordinates
)
# Define a list of states and their corresponding coordinates
states <- list(
California = c(lon = -119.4179, lat = 36.7783),  # California coordinates
Texas = c(lon = -99.9018, lat = 31.9686),        # Texas coordinates
Florida = c(lon = -81.5158, lat = 27.7663),      # Florida coordinates
Colorado = c(lon = -105.7821, lat = 39.5501),    # Colorado coordinates
Arizona = c(lon = -111.0937, lat = 34.0489),     # Arizona coordinates
Oregon = c(lon = -120.5542, lat = 43.8041),      # Oregon coordinates
Washington = c(lon = -120.7401, lat = 47.7511)   # Washington coordinates
)
date_start <- "2020-01-01"
date_end <- "2020-12-31"
# Specify the weather variable you want (e.g., maximum temperature 'TMAX')
weather_variable <- "TMAX"
# Initialize a list to store temperature data for each state
temperature_data_list <- list()
# Loop through the states and retrieve historical temperature data
for (state_name in names(states)) {
# Retrieve historical temperature data for the current state
temperature_data <- meteo_pull_monitors(
station_id = meteo_nearby_stations(states[[state_name]], var = weather_variable)$id[1],
var = weather_variable,
date_min = date_start,
date_max = date_end
)
# Add the temperature data to the list with state name as the key
temperature_data_list[[state_name]] <- temperature_data
}
View(states)
# Print the retrieved temperature data for each state
for (state_name in names(temperature_data_list)) {
cat(state_name, "Temperature Data:\n")
print(temperature_data_list[[state_name]])
cat("\n")
}
# Calculate the cumulative probability P(X ≤ x)
cumulative_probability <- pbinom(x, n, p)
n <- 560  # Total number of posts
p <- 0.35  # Probability that a post mentions a location
x <- 65  # Number of posts to calculate the probability for
# Calculate the cumulative probability P(X ≤ x)
cumulative_probability <- pbinom(x, n, p)
n <- 560  # Total number of posts
p <- 0.35  # Probability that a post mentions a location
x <- 65  # Number of posts to calculate the probability for
# Calculate the cumulative probability P(X ≤ x)
pbinom(x, n, p)
# Define the parameters
n <- 560  # Total number of posts
p <- 0.35  # Probability that a post mentions a location
x <- 65  # Number of posts to calculate the probability for
# Calculate the cumulative probability P(X ≤ x)
cumulative_probability <- pbinom(x, n, p)
# Print the result rounded to two decimal places
round(cumulative_probability, 2)
library(httr)
library(jsonlite)
library(stringr)
library(tm)
library(dplyr)
baseURL <- "https://newsapi.org/v2/everything?"
total_requests <- 2
verbose <- TRUE
API_KEY <- '7623798eeb714a3083e6e9f49af33fc3'
TOPIC <- 'wildfire prevention'
URLpost <- list(
apiKey = API_KEY,
q = paste('+', TOPIC),
sortBy = 'relevancy',
totalRequests = total_requests
)
# GET DATA FROM API
response <- GET(baseURL, query = URLpost)
response_json <- content(response, as = "text")
response_json <- content(response)
View(response)
# GET DATA FROM API
response <- GET(baseURL, query = URLpost)
# Check for errors
if (http_error(response)) {
stop("HTTP request error: ", http_status(response)$reason)
}
# Convert the response to text and then parse it as JSON
response_text <- content(response, as = "text")
# Convert the response to text and then parse it as JSON
response_text <- content(response, "text")
?content()
# Extract the content as text
response_text <- readLines(textConnection(content(response)))
response_raw <- content(response, "raw")
print(response)
# Parse the JSON response
response_data <- fromJSON(content(response, "text"))
# Parse the JSON response
response_data <- fromJSON(response)
articles <- response$articles
response_data = rawToChar(news$content)
response_data = rawToChar(response$content)
View(response)
jsondata = fromJSON(response_data )
articles <- response$articles
# GET DATA FROM API
response <- GET(baseURL, query = URLpost)
response_text = rawToChar(response$content)
response_data = fromJSON(response_text)
articles <- response_data$articles
print(articles)
# Function to clean strings
string_cleaner <- function(input_string) {
out <- ""
tryCatch({
# Replace punctuation and multiple spaces
out <- str_replace_all(input_string, "[,.;@#?!&$-]+", " ")
# Replace select characters with nothing
out <- str_replace_all(out, "[’]+", "")
# Eliminate duplicate whitespaces
out <- str_replace_all(out, "\\s+", " ")
# Convert to lowercase
out <- tolower(out)
}, error = function(e) {
cat("ERROR\n")
out <- ""
})
return(out)
}
article_keys <- names(articles[[1]])
print("AVAILABLE KEYS:")
print(article_keys)
index <- 0
cleaned_data <- list()
for (article in articles) {
tmp <- list()
if (verbose) {
cat("#------------------------------------------\n")
cat("#", index, "\n")
cat("#------------------------------------------\n")
}
for (key in article_keys) {
if (verbose) {
cat("----------------\n")
cat(key, "\n")
cat(article[[key]], "\n")
cat("----------------\n")
}
if (key == 'source') {
src <- string_cleaner(article[[key]]$name)
tmp <- c(tmp, src)
}
if (key == 'author') {
author <- string_cleaner(article[[key]])
# ERROR CHECK (SOMETIMES AUTHOR IS SAME AS PUBLICATION)
if (src %in% author) {
cat(" AUTHOR ERROR:", author)
author <- 'NA'
}
tmp <- c(tmp, author)
}
if (key == 'title') {
tmp <- c(tmp, string_cleaner(article[[key]]))
}
if (key == 'description') {
tmp <- c(tmp, string_cleaner(article[[key]]))
}
if (key == 'content') {
tmp <- c(tmp, string_cleaner(article[[key]]))
}
if (key == 'publishedAt') {
# DEFINE DATA PATTERN FOR REGEX TO CHECK  .* --> wildcard
ref <- ".*-.*-.*T.*:.*:.*Z"
date <- article[[key]]
if (!grepl(ref, date)) {
cat(" DATE ERROR:", date)
date <- "NA"
}
tmp <- c(tmp, date)
}
}
cleaned_data <- append(cleaned_data, list(tmp))
index <- index + 1
}
getwd()
library(httr)
library(jsonlite)
library(stringr)
library(tm)
library(dplyr)
baseURL <- "https://newsapi.org/v2/everything?"
total_requests <- 2
verbose <- TRUE
API_KEY <- '7623798eeb714a3083e6e9f49af33fc3'
TOPIC <- 'wildfire prevention'
URLpost <- list(
apiKey = API_KEY,
q = paste('+', TOPIC),
sortBy = 'relevancy',
totalRequests = total_requests
)
# GET DATA FROM API
response <- GET(baseURL, query = URLpost)
response_text = rawToChar(response$content)
response_data = fromJSON(response_text)
articles <- response_data$articles
print(articles)
# Save articles data to a text file
write.table(articles, file = "articles.txt", quote = FALSE, row.names = FALSE, col.names = TRUE, sep = "\t")
# Save articles data to a text file
write.table(articles, file = "articles.json", quote = FALSE, row.names = FALSE, col.names = TRUE, sep = "\t")
library(httr)
library(jsonlite)
library(stringr)
library(tm)
library(dplyr)
baseURL <- "https://newsapi.org/v2/everything?"
total_requests <- 2
verbose <- TRUE
API_KEY <- '7623798eeb714a3083e6e9f49af33fc3'
TOPIC <- 'wildfire prevention'
URLpost <- list(
apiKey = API_KEY,
q = paste('+', TOPIC),
sortBy = 'relevancy',
totalRequests = total_requests
)
# GET DATA FROM API
response <- GET(baseURL, query = URLpost)
response_text = rawToChar(response$content)
response_data = fromJSON(response_text)
articles <- response_data$articles
print(articles)
getwd()
# Save articles data to a text file
setwd("../../data/raw-data")
write_json(articles, file = "articles.json")
getwd()
write_json(articles, path = "articles.json")
library(httr)
library(jsonlite)
library(stringr)
library(tm)
library(dplyr)
baseURL <- "https://newsapi.org/v2/everything?"
total_requests <- 2
verbose <- TRUE
API_KEY <- '7623798eeb714a3083e6e9f49af33fc3'
TOPIC <- 'wildfire prevention'
URLpost <- list(
apiKey = API_KEY,
q = paste('+', TOPIC),
sortBy = 'relevancy',
totalRequests = total_requests
)
# GET DATA FROM API
response <- GET(baseURL, query = URLpost)
response_text = rawToChar(response$content)
response_data = fromJSON(response_text)
articles <- response_data$articles
print(articles)
# Save articles data to a text file
setwd("../../data/raw-data")
write_json(articles, path = "articles.json")
